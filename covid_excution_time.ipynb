{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e827",
   "metadata": {},
   "source": [
    "# Excution time measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9f2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2609f",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b56be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d1a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = 'output/data1.csv'\n",
    "data2 = 'output/data2.csv'\n",
    "\n",
    "\n",
    "\n",
    "file_pathss = [\n",
    "    'output/data1.csv',\n",
    "    'output/data2.csv',\n",
    "    'output/data3.csv',\n",
    "    'output/data4.csv'\n",
    "]\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    'output/data1.csv',\n",
    "    'output/data2.csv',\n",
    "    'output/data3.csv',\n",
    "    'output/data4.csv',\n",
    "    'output/data5.csv',\n",
    "    'output/data6.csv',\n",
    "    'output/data7.csv',\n",
    "    'output/data8.csv',\n",
    "    'output/data9.csv',\n",
    "    'output/data10.csv',\n",
    "    'output/data12.csv',\n",
    "    'output/data13.csv',\n",
    "    'output/data14.csv',\n",
    "    'output/data15.csv',\n",
    "]\n",
    "\n",
    "bigger_files = [\n",
    "    'output/data16.csv',\n",
    "    'output/data17.csv',\n",
    "    'output/data18.csv',\n",
    "    'output/data19.csv',\n",
    "    'output/data20.csv',\n",
    "    'output/data21.csv',\n",
    "    'output/data22.csv',\n",
    "    'output/data23.csv',\n",
    "    'output/data24.csv'\n",
    "]\n",
    "\n",
    "\n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f893df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(data1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7eb55",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ecf664af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(x):\n",
    "    df = pd.read_csv(x)\n",
    "    return df\n",
    "\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_read_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_pandas = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_loading(paths):\n",
    "    avg_times = measure_average_read_time(paths)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average read time for data_{i}: {avg_time:.5f} seconds\")\n",
    "        \n",
    "run_loading(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1092d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average read time for data_1: 25.89928 seconds\n",
      "Average read time for data_2: 39.33730 seconds\n",
      "Average read time for data_3: 48.52571 seconds\n",
      "Average read time for data_4: 60.61468 seconds\n",
      "Average read time for data_5: 70.24155 seconds\n",
      "Average read time for data_6: 76.79467 seconds\n",
      "Average read time for data_7: 83.14161 seconds\n",
      "Average read time for data_8: 94.47625 seconds\n",
      "Average read time for data_9: 106.77703 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measuring lead time of bigger datasize - 6Gb - 15 GB\n",
    "run_loading(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dce816",
   "metadata": {},
   "source": [
    "# Filtering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0ae62f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 0.00349 seconds\n",
      "Average filter time for data-2: 0.00473 seconds\n",
      "Average filter time for data-3: 0.00702 seconds\n",
      "Average filter time for data-4: 0.00936 seconds\n",
      "Average filter time for data-5: 0.01178 seconds\n",
      "Average filter time for data-6: 0.01631 seconds\n",
      "Average filter time for data-7: 0.01802 seconds\n",
      "Average filter time for data-8: 0.01983 seconds\n",
      "Average filter time for data-9: 0.03158 seconds\n",
      "Average filter time for data-10: 0.03112 seconds\n",
      "Average filter time for data-11: 0.08747 seconds\n",
      "Average filter time for data-12: 0.18891 seconds\n",
      "Average filter time for data-13: 0.72862 seconds\n",
      "Average filter time for data-14: 1.95690 seconds\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"output/data1.csv\")\n",
    "\n",
    "addresses = [\"output/data1.csv\"]\n",
    "\n",
    "# we are filtering from a column \"5.164162635803223\" which are in values b/n and including 6 and 7.\n",
    "def filter_and_select(filter_it):\n",
    "    return filter_it.loc[(filter_it[\"5.164162635803223\"] >= 6) & (filter_it[\"5.164162635803223\"] <= 7)]\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def filter_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = filter_and_select(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "def measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = pd.read_csv(file_path)\n",
    "        timings_pandas = [filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_filter(addresses):\n",
    "    avg_times = measure_average_filter_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "        \n",
    "run_filter(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cde0405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 1.55108 seconds\n",
      "Average filter time for data-2: 2.82007 seconds\n",
      "Average filter time for data-3: 4.15704 seconds\n",
      "Average filter time for data-4: 5.44873 seconds\n",
      "Average filter time for data-5: 7.53639 seconds\n",
      "Average filter time for data-6: 8.85096 seconds\n",
      "Average filter time for data-7: 13.64158 seconds\n",
      "Average filter time for data-8: 55.19761 seconds\n",
      "Average filter time for data-9: 56.10705 seconds\n"
     ]
    }
   ],
   "source": [
    "run_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dbdae",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17c3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3029ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f187056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ec9605",
   "metadata": {},
   "source": [
    "<h1>Spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c62c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, mean, count, udf\n",
    "import timeit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41285332",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6bda05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 12:14:39 WARN Utils: Your hostname, Merons-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.103 instead (on interface en0)\n",
      "23/06/08 12:14:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 12:14:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_csv(x):\n",
    "    df = spark.read.option('header', 'true').csv(x, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_read_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_loading_spark(addresses):\n",
    "    avg_times = measure_average_read_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average read time for data_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "347f643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2006:==============================================>       (33 + 5) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average read time for data_1: 1.57070 seconds\n",
      "Average read time for data_2: 2.80850 seconds\n",
      "Average read time for data_3: 4.16848 seconds\n",
      "Average read time for data_4: 5.90993 seconds\n",
      "Average read time for data_5: 7.11664 seconds\n",
      "Average read time for data_6: 8.18952 seconds\n",
      "Average read time for data_7: 9.38334 seconds\n",
      "Average read time for data_8: 10.13698 seconds\n",
      "Average read time for data_9: 11.67882 seconds\n",
      "Average read time for data_10: 13.04607 seconds\n",
      "Average read time for data_11: 27.06401 seconds\n",
      "Average read time for data_12: 40.34847 seconds\n",
      "Average read time for data_13: 53.89822 seconds\n",
      "Average read time for data_14: 67.64213 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_loading_spark(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e92f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:====================================================>(104 + 1) / 105]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average read time for data_1: 104.29503 seconds\n",
      "Average read time for data_2: 132.03032 seconds\n",
      "Average read time for data_3: 149.14590 seconds\n",
      "Average read time for data_4: 170.31554 seconds\n",
      "Average read time for data_5: 233.77961 seconds\n",
      "Average read time for data_6: 305.62885 seconds\n",
      "Average read time for data_7: 249.09140 seconds\n",
      "Average read time for data_8: 277.08775 seconds\n",
      "Average read time for data_9: 341.47879 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_loading_spark(bigger_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edf280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68735a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d2b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c03e9ab8",
   "metadata": {},
   "source": [
    "# Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76873907",
   "metadata": {},
   "source": [
    "# Trying to filter without action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d25093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# We are filtering from a column \"5.164162635803223\" which has values between and including 6 and 7.\n",
    "def spark_filter_and_select(df):\n",
    "    return df.filter((df[\"`5.164162635803223`\"] >= 6) & (df[\"`5.164162635803223`\"] <= 7))\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def spark_filter_time(df):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = spark_filter_and_select(df)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def spark_measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = spark.read.option('header', 'true').csv(file_path, inferSchema=True)\n",
    "        timings_spark = [spark_filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_spark_filter(filesss):\n",
    "    avg_times = spark_measure_average_filter_time(filesss)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        if i >= 11:\n",
    "            print(f\"Average filter time for data-{i + 1}: {avg_time:.5f} seconds\")\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08763dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 207:===============================================>       (33 + 5) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 0.08031 seconds\n",
      "Average filter time for data-2: 0.04739 seconds\n",
      "Average filter time for data-3: 0.04467 seconds\n",
      "Average filter time for data-4: 0.04481 seconds\n",
      "Average filter time for data-5: 0.04563 seconds\n",
      "Average filter time for data-6: 0.04437 seconds\n",
      "Average filter time for data-7: 0.04468 seconds\n",
      "Average filter time for data-8: 0.04431 seconds\n",
      "Average filter time for data-9: 0.04232 seconds\n",
      "Average filter time for data-10: 0.04756 seconds\n",
      "Average filter time for data-12: 0.04363 seconds\n",
      "Average filter time for data-11: 0.04363 seconds\n",
      "Average filter time for data-13: 0.04366 seconds\n",
      "Average filter time for data-12: 0.04366 seconds\n",
      "Average filter time for data-14: 0.04450 seconds\n",
      "Average filter time for data-13: 0.04450 seconds\n",
      "Average filter time for data-15: 0.04369 seconds\n",
      "Average filter time for data-14: 0.04369 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_spark_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d747912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 225:====================================================>(104 + 1) / 105]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 0.04396 seconds\n",
      "Average filter time for data-2: 0.04485 seconds\n",
      "Average filter time for data-3: 0.04469 seconds\n",
      "Average filter time for data-4: 0.04246 seconds\n",
      "Average filter time for data-5: 0.03135 seconds\n",
      "Average filter time for data-6: 0.03210 seconds\n",
      "Average filter time for data-7: 0.04341 seconds\n",
      "Average filter time for data-8: 0.02842 seconds\n",
      "Average filter time for data-9: 0.02528 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_spark_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba48e",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40553963",
   "metadata": {},
   "source": [
    "# Trying to filter with an actual action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39dee48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# We are filtering from a column \"5.164162635803223\" which has values between and including 6 and 7.\n",
    "def spark_filter_and_select(df):\n",
    "    return df.filter((df[\"`5.164162635803223`\"] >= 6) & (df[\"`5.164162635803223`\"] <= 7)).count()\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def spark_filter_time(df):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = spark_filter_and_select(df)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def spark_measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = spark.read.option('header', 'true').csv(file_path, inferSchema=True)\n",
    "        timings_spark = [spark_filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_spark_filter(filesss):\n",
    "    avg_times = spark_measure_average_filter_time(filesss)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        if i >= 11:\n",
    "            print(f\"Average filter time for data-{i + 1}: {avg_time:.5f} seconds\")\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53782524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1087:==============================================>       (33 + 5) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 0.59102 seconds\n",
      "Average filter time for data-2: 1.02199 seconds\n",
      "Average filter time for data-3: 1.89531 seconds\n",
      "Average filter time for data-4: 4.22321 seconds\n",
      "Average filter time for data-5: 5.73304 seconds\n",
      "Average filter time for data-6: 6.51344 seconds\n",
      "Average filter time for data-7: 8.08491 seconds\n",
      "Average filter time for data-8: 9.38493 seconds\n",
      "Average filter time for data-9: 10.75522 seconds\n",
      "Average filter time for data-10: 11.46883 seconds\n",
      "Average filter time for data-12: 15.49780 seconds\n",
      "Average filter time for data-11: 15.49780 seconds\n",
      "Average filter time for data-13: 16.79853 seconds\n",
      "Average filter time for data-12: 16.79853 seconds\n",
      "Average filter time for data-14: 18.25365 seconds\n",
      "Average filter time for data-13: 18.25365 seconds\n",
      "Average filter time for data-15: 27.86440 seconds\n",
      "Average filter time for data-14: 27.86440 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_spark_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7892bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 639:=================================================>    (96 + 8) / 105]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter time for data-1: 37.22346 seconds\n",
      "Average filter time for data-2: 56.07037 seconds\n",
      "Average filter time for data-3: 61.17328 seconds\n",
      "Average filter time for data-4: 70.76579 seconds\n",
      "Average filter time for data-5: 79.21383 seconds\n",
      "Average filter time for data-6: 73.63960 seconds\n",
      "Average filter time for data-7: 52.57020 seconds\n",
      "Average filter time for data-8: 63.80649 seconds\n",
      "Average filter time for data-9: 73.58435 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_spark_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf28a89",
   "metadata": {},
   "source": [
    "# And for that reason, we also doing the results for pandas with counting of rows included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd6a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"output/data1.csv\")\n",
    "\n",
    "addresses = [\"output/data1.csv\"]\n",
    "\n",
    "# we are filtering from a column \"5.164162635803223\" which are in values b/n and including 6 and 7.\n",
    "def filter_and_select(filter_it):\n",
    "    return (filter_it.loc[(filter_it[\"5.164162635803223\"] >= 6) & (filter_it[\"5.164162635803223\"] <= 7)]).shape[0]\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def filter_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = filter_and_select(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "def measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = pd.read_csv(file_path)\n",
    "        timings_pandas = [filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_filter(addresses):\n",
    "    avg_times = measure_average_filter_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Pandas - Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868f71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Average filter time for data-1: 0.01468 seconds\n",
      "Pandas - Average filter time for data-2: 0.01710 seconds\n",
      "Pandas - Average filter time for data-3: 0.02160 seconds\n",
      "Pandas - Average filter time for data-4: 0.03065 seconds\n",
      "Pandas - Average filter time for data-5: 0.04493 seconds\n",
      "Pandas - Average filter time for data-6: 0.03973 seconds\n",
      "Pandas - Average filter time for data-7: 0.05440 seconds\n",
      "Pandas - Average filter time for data-8: 0.06208 seconds\n",
      "Pandas - Average filter time for data-9: 0.06275 seconds\n",
      "Pandas - Average filter time for data-10: 0.06771 seconds\n",
      "Pandas - Average filter time for data-11: 0.16391 seconds\n",
      "Pandas - Average filter time for data-12: 0.36188 seconds\n",
      "Pandas - Average filter time for data-13: 0.59572 seconds\n",
      "Pandas - Average filter time for data-14: 0.82604 seconds\n"
     ]
    }
   ],
   "source": [
    "run_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4e4914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Average filter time for data-1: 0.90013 seconds\n",
      "Pandas - Average filter time for data-2: 4.16967 seconds\n",
      "Pandas - Average filter time for data-3: 5.31762 seconds\n",
      "Pandas - Average filter time for data-4: 6.70582 seconds\n",
      "Pandas - Average filter time for data-5: 11.42966 seconds\n",
      "Pandas - Average filter time for data-6: 25.04504 seconds\n",
      "Pandas - Average filter time for data-7: 17.40607 seconds\n",
      "Pandas - Average filter time for data-8: 91.89603 seconds\n",
      "Pandas - Average filter time for data-9: 139.21290 seconds\n"
     ]
    }
   ],
   "source": [
    "run_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee5843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799f6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79950ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cc5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
