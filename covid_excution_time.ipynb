{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e827",
   "metadata": {},
   "source": [
    "# Excution time measurements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9f2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2609f",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b56be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d1a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = 'output/data1.csv'\n",
    "data2 = 'output/data2.csv'\n",
    "biggest = 'output/data2.csv'\n",
    "\n",
    "\n",
    "\n",
    "file_pathss = [\n",
    "    'output/data1.csv',\n",
    "    'output/data2.csv',\n",
    "    'output/data3.csv',\n",
    "    'output/data4.csv'\n",
    "]\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    'output/data1.csv',\n",
    "    'output/data2.csv',\n",
    "    'output/data3.csv',\n",
    "    'output/data4.csv',\n",
    "    'output/data5.csv',\n",
    "    'output/data6.csv',\n",
    "    'output/data7.csv',\n",
    "    'output/data8.csv',\n",
    "    'output/data9.csv',\n",
    "    'output/data10.csv',\n",
    "    'output/data12.csv',\n",
    "    'output/data13.csv',\n",
    "    'output/data14.csv',\n",
    "    'output/data15.csv',\n",
    "]\n",
    "\n",
    "bigger_files = [\n",
    "    'output/data16.csv',\n",
    "    'output/data17.csv',\n",
    "    'output/data18.csv',\n",
    "    'output/data19.csv',\n",
    "    'output/data20.csv',\n",
    "    'output/data21.csv',\n",
    "    'output/data22.csv',\n",
    "    'output/data23.csv',\n",
    "    'output/data24.csv'\n",
    "]\n",
    "\n",
    "\n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f893df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(data1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7eb55",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ecf664af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(x):\n",
    "    df = pd.read_csv(x)\n",
    "    return df\n",
    "\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_read_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_pandas = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_loading(paths):\n",
    "    avg_times = measure_average_read_time(paths)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average read time for data_{i}: {avg_time:.5f} seconds\")\n",
    "        \n",
    "run_loading(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1092d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring lead time of bigger datasize - 6Gb - 15 GB\n",
    "#run_loading(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dce816",
   "metadata": {},
   "source": [
    "# Filtering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae62f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"output/data1.csv\")\n",
    "\n",
    "addresses = [\"output/data1.csv\"]\n",
    "\n",
    "# we are filtering from a column \"5.164162635803223\" which are in values b/n and including 6 and 7.\n",
    "def filter_and_select(filter_it):\n",
    "    return filter_it.loc[(filter_it[\"5.164162635803223\"] >= 6) & (filter_it[\"5.164162635803223\"] <= 7)]\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def filter_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = filter_and_select(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "def measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = pd.read_csv(file_path)\n",
    "        timings_pandas = [filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_filter(addresses):\n",
    "    avg_times = measure_average_filter_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "        \n",
    "#run_filter(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde0405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dbdae",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17c3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3029ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f187056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ec9605",
   "metadata": {},
   "source": [
    "<h1>Spark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c62c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, mean, count, udf\n",
    "import timeit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41285332",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6bda05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/11 20:53:43 WARN Utils: Your hostname, Merons-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.103 instead (on interface en0)\n",
      "23/06/11 20:53:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/11 20:53:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/11 20:53:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_csv(x):\n",
    "    df = spark.read.option('header', 'true').csv(x, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_read_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_loading_spark(addresses):\n",
    "    avg_times = measure_average_read_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average read time for data_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347f643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_loading_spark(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e92f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_loading_spark(bigger_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edf280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68735a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d2b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c03e9ab8",
   "metadata": {},
   "source": [
    "# Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76873907",
   "metadata": {},
   "source": [
    "# Trying to filter without action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d25093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# We are filtering from a column \"5.164162635803223\" which has values between and including 6 and 7.\n",
    "def spark_filter_and_select(df):\n",
    "    return df.filter((df[\"`5.164162635803223`\"] >= 6) & (df[\"`5.164162635803223`\"] <= 7))\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def spark_filter_time(df):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = spark_filter_and_select(df)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def spark_measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = spark.read.option('header', 'true').csv(file_path, inferSchema=True)\n",
    "        timings_spark = [spark_filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_spark_filter(filesss):\n",
    "    avg_times = spark_measure_average_filter_time(filesss)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        if i >= 11:\n",
    "            print(f\"Average filter time for data-{i + 1}: {avg_time:.5f} seconds\")\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08763dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_spark_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d747912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_spark_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba48e",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40553963",
   "metadata": {},
   "source": [
    "# Trying to filter with an actual action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39dee48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import timeit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# We are filtering from a column \"5.164162635803223\" which has values between and including 6 and 7.\n",
    "def spark_filter_and_select(df):\n",
    "    return df.filter((df[\"`5.164162635803223`\"] >= 6) & (df[\"`5.164162635803223`\"] <= 7)).count()\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def spark_filter_time(df):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = spark_filter_and_select(df)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def spark_measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = spark.read.option('header', 'true').csv(file_path, inferSchema=True)\n",
    "        timings_spark = [spark_filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "def run_spark_filter(filesss):\n",
    "    avg_times = spark_measure_average_filter_time(filesss)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Average filter time for data-{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53782524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_spark_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7892bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_spark_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf28a89",
   "metadata": {},
   "source": [
    "# And for that reason, we also doing the results for pandas with counting of rows included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd6a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"output/data1.csv\")\n",
    "\n",
    "addresses = [\"output/data1.csv\"]\n",
    "\n",
    "# we are filtering from a column \"5.164162635803223\" which are in values b/n and including 6 and 7.\n",
    "def filter_and_select(filter_it):\n",
    "    return (filter_it.loc[(filter_it[\"5.164162635803223\"] >= 6) & (filter_it[\"5.164162635803223\"] <= 7)]).shape[0]\n",
    "\n",
    "# Measurement - Filter_csv\n",
    "def filter_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = filter_and_select(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "def measure_average_filter_time(files, cycles=10):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        local_df = pd.read_csv(file_path)\n",
    "        timings_pandas = [filter_time(local_df) for _ in range(cycles)]\n",
    "        avg_time_pandas = average_time(timings_pandas)\n",
    "        avg_times.append(avg_time_pandas)\n",
    "    return avg_times\n",
    "\n",
    "def run_filter(addresses):\n",
    "    avg_times = measure_average_filter_time(addresses)\n",
    "\n",
    "    for i, avg_time in enumerate(avg_times, 1):\n",
    "        print(f\"Pandas - Average filter time for data-{i}: {avg_time:.5f} seconds\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "868f71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_filter(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4e4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_filter(bigger_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee5843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799f6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79950ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cc5e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
