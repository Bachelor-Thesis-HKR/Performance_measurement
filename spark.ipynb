{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7649f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, mean, count, udf\n",
    "import timeit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e5cc50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m email_5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplitted/emails_part_5.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m email_6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplitted/emails_part_6.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(email_1, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 37  MB\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df2 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(email_2, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 79  MB\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df3 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(email_3, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 139 MB\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "cycles = 10\n",
    "\n",
    "email_1 = 'splitted/emails_part_1.csv'\n",
    "email_2 = 'splitted/emails_part_2.csv'\n",
    "email_3 = 'splitted/emails_part_3.csv'\n",
    "email_4 = 'splitted/emails_part_4.csv'\n",
    "email_5 = 'splitted/emails_part_5.csv'\n",
    "email_6 = 'splitted/emails_part_6.csv'\n",
    "\n",
    "\n",
    "df1 = spark.read.option('header', 'true').csv(email_1, inferSchema=True) # 37  MB\n",
    "df2 = spark.read.option('header', 'true').csv(email_2, inferSchema=True) # 79  MB\n",
    "df3 = spark.read.option('header', 'true').csv(email_3, inferSchema=True) # 139 MB\n",
    "df4 = spark.read.option('header', 'true').csv(email_4, inferSchema=True) # 331 MB\n",
    "df5 = spark.read.option('header', 'true').csv(email_5, inferSchema=True) # 599 MB\n",
    "df6 = spark.read.option('header', 'true').csv(email_6, inferSchema=True) # 1.08 GB\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "\n",
    "file_paths = [\n",
    "    'splitted/emails_part_1.csv',\n",
    "    'splitted/emails_part_2.csv',\n",
    "    'splitted/emails_part_3.csv',\n",
    "    'splitted/emails_part_4.csv',\n",
    "    'splitted/emails_part_5.csv',\n",
    "    'splitted/emails_part_6.csv'\n",
    "]\n",
    "\n",
    "\n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6297ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/04 16:15:03 WARN Utils: Your hostname, Merons-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.103 instead (on interface en0)\n",
      "23/06/04 16:15:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/04 16:15:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>emails_session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff41960f9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"emails_session\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca4975",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "103d7113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meron/anaconda3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = spark.read.option('header', 'true').csv(email_1, inferSchema=True) # 37  MB\n",
    "sample = df2.sample(fraction = 0.01)\n",
    "pdf = sample.toPandas()\n",
    "\n",
    "\n",
    "def memory_usage(df):\n",
    "    return(round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2))\n",
    "\n",
    "memory_usage(pdf) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a849e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 762:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average read time for email_1: 0.02039 seconds\n",
      "Average read time for email_2: 0.01532 seconds\n",
      "Average read time for email_3: 0.01878 seconds\n",
      "Average read time for email_4: 0.04353 seconds\n",
      "Average read time for email_5: 0.06368 seconds\n",
      "Average read time for email_6: 0.10706 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "\n",
    "# Read the CSV file\n",
    "def read_csv(x):\n",
    "    df = spark.read.option('header', 'true').csv(x, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "# Measurement - read_csv\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "    \n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time\n",
    "\n",
    "def measure_average_read_time(files, cycles=1):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "avg_times = measure_average_read_time(file_paths)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average read time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568efbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70ad694",
   "metadata": {},
   "source": [
    "# Filtering and selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "652b9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter and select time for email_1 with sender 'john.arnold@enron.com': 0.00317 seconds\n",
      "Average filter and select time for email_2 with sender 'john.arnold@enron.com': 0.00176 seconds\n",
      "Average filter and select time for email_3 with sender 'john.arnold@enron.com': 0.00138 seconds\n",
      "Average filter and select time for email_4 with sender 'john.arnold@enron.com': 0.00113 seconds\n",
      "Average filter and select time for email_5 with sender 'john.arnold@enron.com': 0.00104 seconds\n",
      "Average filter and select time for email_6 with sender 'john.arnold@enron.com': 0.00086 seconds\n"
     ]
    }
   ],
   "source": [
    "def filter_and_select_spark(df, sender=None):\n",
    "    filtered_df = df\n",
    "    if sender:\n",
    "        filtered_df = filtered_df.filter(col(\"From\") == sender) \n",
    "    return filtered_df\n",
    "\n",
    "def filter_and_select_time_spark(file_path, sender):\n",
    "    start_time = timeit.default_timer()\n",
    "    filtered_df = filter_and_select_spark(df, sender)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_filter_and_select_time_spark(files, sender):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [filter_and_select_time_spark(file_path, sender) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "\n",
    "sender = \"john.arnold@enron.com\"\n",
    "avg_times = measure_average_filter_and_select_time_spark(dfs, sender)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average filter and select time for email_{i} with sender '{sender}': {avg_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a6f75",
   "metadata": {},
   "source": [
    "# GroupBy and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e7cc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average groupby and aggregate time for email_1: 0.00488 seconds\n",
      "Average groupby and aggregate time for email_2: 0.00281 seconds\n",
      "Average groupby and aggregate time for email_3: 0.00222 seconds\n",
      "Average groupby and aggregate time for email_4: 0.00259 seconds\n",
      "Average groupby and aggregate time for email_5: 0.00256 seconds\n",
      "Average groupby and aggregate time for email_6: 0.00249 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def groupby_and_aggregate_spark(df, groupby_column, agg_function):\n",
    "    return df.groupby(groupby_column).agg(agg_function)\n",
    "\n",
    "def groupby_and_aggregate_time_spark(file_path, groupby_column, agg_function):\n",
    "    start_time = timeit.default_timer()\n",
    "    grouped_df = groupby_and_aggregate_spark(df, groupby_column, agg_function)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_groupby_and_aggregate_time_spark(files, groupby_column, agg_function):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [groupby_and_aggregate_time_spark(file_path, groupby_column, agg_function) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "groupby_column = \"From\"\n",
    "agg_function = {\"Date\": \"count\"}\n",
    "\n",
    "avg_times = measure_average_groupby_and_aggregate_time_spark(dfs, groupby_column, agg_function)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average groupby and aggregate time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a432d31",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6058b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average data transformation time for email_1: 0.00851 seconds\n",
      "Average data transformation time for email_2: 0.00657 seconds\n",
      "Average data transformation time for email_3: 0.00756 seconds\n",
      "Average data transformation time for email_4: 0.00662 seconds\n",
      "Average data transformation time for email_5: 0.00710 seconds\n",
      "Average data transformation time for email_6: 0.00498 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_data_spark(df, column, transformation_function):\n",
    "    udf_transform = udf(transformation_function, StringType())\n",
    "    return df.withColumn(column, udf_transform(col(column)))\n",
    "\n",
    "# Data transformation example\n",
    "def uppercase_subject_spark(subject):\n",
    "    if subject:\n",
    "        return subject.upper()\n",
    "    return subject\n",
    "\n",
    "\n",
    "def transform_data_time_spark(file_path, column, transformation_function):\n",
    "    start_time = timeit.default_timer()\n",
    "    transformed_df = transform_data_spark(df, column, transformation_function)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_transform_data_time_spark(files, column, transformation_function):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [transform_data_time_spark(file_path, column, transformation_function) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "column = \"Subject\"\n",
    "\n",
    "avg_times = measure_average_transform_data_time_spark(dfs, column, uppercase_subject_spark)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average data transformation time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c3236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
