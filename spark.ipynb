{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7649f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, mean, count, udf\n",
    "import timeit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "86e5cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cycles = 10\n",
    "\n",
    "email_1 = 'splitted/emails_part_1.csv'\n",
    "email_2 = 'splitted/emails_part_2.csv'\n",
    "email_3 = 'splitted/emails_part_3.csv'\n",
    "email_4 = 'splitted/emails_part_4.csv'\n",
    "email_5 = 'splitted/emails_part_5.csv'\n",
    "email_6 = 'splitted/emails_part_6.csv'\n",
    "\n",
    "\n",
    "df1 = spark.read.option('header', 'true').csv(email_1, inferSchema=True) # 37  MB\n",
    "df2 = spark.read.option('header', 'true').csv(email_2, inferSchema=True) # 79  MB\n",
    "df3 = spark.read.option('header', 'true').csv(email_3, inferSchema=True) # 139 MB\n",
    "df4 = spark.read.option('header', 'true').csv(email_4, inferSchema=True) # 331 MB\n",
    "df5 = spark.read.option('header', 'true').csv(email_5, inferSchema=True) # 599 MB\n",
    "df6 = spark.read.option('header', 'true').csv(email_6, inferSchema=True) # 1.08 GB\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "\n",
    "file_paths = [\n",
    "    'splitted/emails_part_1.csv',\n",
    "    'splitted/emails_part_2.csv',\n",
    "    'splitted/emails_part_3.csv',\n",
    "    'splitted/emails_part_4.csv',\n",
    "    'splitted/emails_part_5.csv',\n",
    "    'splitted/emails_part_6.csv'\n",
    "]\n",
    "\n",
    "\n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c6297ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.106:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>emails_session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7ae844f220>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"emails_session\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca4975",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a849e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 762:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average read time for email_1: 0.02039 seconds\n",
      "Average read time for email_2: 0.01532 seconds\n",
      "Average read time for email_3: 0.01878 seconds\n",
      "Average read time for email_4: 0.04353 seconds\n",
      "Average read time for email_5: 0.06368 seconds\n",
      "Average read time for email_6: 0.10706 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "\n",
    "# Read the CSV file\n",
    "def read_csv(x):\n",
    "    df = spark.read.option('header', 'true').csv(x, inferSchema=True)\n",
    "    return df\n",
    "\n",
    "# Measurement - read_csv\n",
    "def read_time(dis):\n",
    "    start_time = timeit.default_timer()\n",
    "    dataframe = read_csv(dis)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "    \n",
    "def average_time(myList):\n",
    "    total_time_spark = 0\n",
    "    for i in myList:\n",
    "        total_time_spark += i\n",
    "    avg_time = total_time_spark / cycles\n",
    "    return avg_time\n",
    "\n",
    "def measure_average_read_time(files, cycles=1):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [read_time(file_path) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "avg_times = measure_average_read_time(file_paths)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average read time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568efbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e70ad694",
   "metadata": {},
   "source": [
    "# Filtering and selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "652b9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average filter and select time for email_1 with sender 'john.arnold@enron.com': 0.00317 seconds\n",
      "Average filter and select time for email_2 with sender 'john.arnold@enron.com': 0.00176 seconds\n",
      "Average filter and select time for email_3 with sender 'john.arnold@enron.com': 0.00138 seconds\n",
      "Average filter and select time for email_4 with sender 'john.arnold@enron.com': 0.00113 seconds\n",
      "Average filter and select time for email_5 with sender 'john.arnold@enron.com': 0.00104 seconds\n",
      "Average filter and select time for email_6 with sender 'john.arnold@enron.com': 0.00086 seconds\n"
     ]
    }
   ],
   "source": [
    "def filter_and_select_spark(df, sender=None):\n",
    "    filtered_df = df\n",
    "    if sender:\n",
    "        filtered_df = filtered_df.filter(col(\"From\") == sender) \n",
    "    return filtered_df\n",
    "\n",
    "def filter_and_select_time_spark(file_path, sender):\n",
    "    start_time = timeit.default_timer()\n",
    "    filtered_df = filter_and_select_spark(df, sender)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_filter_and_select_time_spark(files, sender):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [filter_and_select_time_spark(file_path, sender) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "\n",
    "sender = \"john.arnold@enron.com\"\n",
    "avg_times = measure_average_filter_and_select_time_spark(dfs, sender)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average filter and select time for email_{i} with sender '{sender}': {avg_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a6f75",
   "metadata": {},
   "source": [
    "# GroupBy and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e7cc29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average groupby and aggregate time for email_1: 0.00488 seconds\n",
      "Average groupby and aggregate time for email_2: 0.00281 seconds\n",
      "Average groupby and aggregate time for email_3: 0.00222 seconds\n",
      "Average groupby and aggregate time for email_4: 0.00259 seconds\n",
      "Average groupby and aggregate time for email_5: 0.00256 seconds\n",
      "Average groupby and aggregate time for email_6: 0.00249 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def groupby_and_aggregate_spark(df, groupby_column, agg_function):\n",
    "    return df.groupby(groupby_column).agg(agg_function)\n",
    "\n",
    "def groupby_and_aggregate_time_spark(file_path, groupby_column, agg_function):\n",
    "    start_time = timeit.default_timer()\n",
    "    grouped_df = groupby_and_aggregate_spark(df, groupby_column, agg_function)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_groupby_and_aggregate_time_spark(files, groupby_column, agg_function):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [groupby_and_aggregate_time_spark(file_path, groupby_column, agg_function) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "groupby_column = \"From\"\n",
    "agg_function = {\"Date\": \"count\"}\n",
    "\n",
    "avg_times = measure_average_groupby_and_aggregate_time_spark(dfs, groupby_column, agg_function)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average groupby and aggregate time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a432d31",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6058b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average data transformation time for email_1: 0.00851 seconds\n",
      "Average data transformation time for email_2: 0.00657 seconds\n",
      "Average data transformation time for email_3: 0.00756 seconds\n",
      "Average data transformation time for email_4: 0.00662 seconds\n",
      "Average data transformation time for email_5: 0.00710 seconds\n",
      "Average data transformation time for email_6: 0.00498 seconds\n"
     ]
    }
   ],
   "source": [
    "def transform_data_spark(df, column, transformation_function):\n",
    "    udf_transform = udf(transformation_function, StringType())\n",
    "    return df.withColumn(column, udf_transform(col(column)))\n",
    "\n",
    "# Data transformation example\n",
    "def uppercase_subject_spark(subject):\n",
    "    if subject:\n",
    "        return subject.upper()\n",
    "    return subject\n",
    "\n",
    "\n",
    "def transform_data_time_spark(file_path, column, transformation_function):\n",
    "    start_time = timeit.default_timer()\n",
    "    transformed_df = transform_data_spark(df, column, transformation_function)\n",
    "    end_time = timeit.default_timer()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def measure_average_transform_data_time_spark(files, column, transformation_function):\n",
    "    avg_times = []\n",
    "    for file_path in files:\n",
    "        timings_spark = [transform_data_time_spark(file_path, column, transformation_function) for _ in range(cycles)]\n",
    "        avg_time_spark = average_time(timings_spark)\n",
    "        avg_times.append(avg_time_spark)\n",
    "    return avg_times\n",
    "\n",
    "column = \"Subject\"\n",
    "\n",
    "avg_times = measure_average_transform_data_time_spark(dfs, column, uppercase_subject_spark)\n",
    "\n",
    "for i, avg_time in enumerate(avg_times, 1):\n",
    "    print(f\"Average data transformation time for email_{i}: {avg_time:.5f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c3236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
